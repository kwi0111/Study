
import numpy as np
from sklearn.datasets import fetch_covtype
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from xgboost import XGBClassifier

# 데이터 로드
x, y = fetch_covtype(return_X_y=True)
y = y - 1
# 데이터 전처리
x_train, x_test, y_train, y_test = train_test_split(
    x, y, random_state=777, train_size=0.8,
)

scaler = MinMaxScaler()
scaler.fit(x_train)
x_train = scaler.transform(x_train)
x_test = scaler.transform(x_test)

parameters = {
    'n_estimators': 1000,
    'learning_rate': 0.01,
    'max_depth': 10,  # 트리 깊이
    'gamma': 0,
    'min_child_weight': 10,
    'subsample': 0.4,
    'colsample_bytree': 0.8,
    'colsample_bylevel': 0.7,
    'colsample_bynode': 1,
    'reg_alpha': 0,
    'reg_lambda': 1,
    'random_state': 3377,
    'verbose': 0,
}

# 모델 정의 및 설정
model = XGBClassifier(**parameters)

# 피처 중요도 계산
model.fit(x_train, y_train)

feature_importances = model.feature_importances_
sorted_indices = np.argsort(feature_importances)[::-1]  # 각 피처의 중요도를 내림차순으로 정렬하여 인덱스를 얻습니다.

# 피처를 하나씩 제거하면서 모델 평가
scores = []
for i in range(len(sorted_indices), 0, -1):
    # 선택된 피처들의 인덱스
    selected_indices = sorted_indices[:i]
    
    # 선택된 피처로 데이터 재구성
    x_train_selected = x_train[:, selected_indices]
    x_test_selected = x_test[:, selected_indices]
    
    # 모델 재훈련
    model.fit(x_train_selected, y_train)
    
    # 모델 평가
    score = model.score(x_test_selected, y_test)
    scores.append((i, score))

# 결과 출력
for i, score in scores:
    print(f"선택된 피처 개수: {i}, 모델 성능: {score}")


'''
선택된 피처 개수: 54, 모델 성능: 0.8886775728681704
선택된 피처 개수: 53, 모델 성능: 0.8880665731521561
선택된 피처 개수: 52, 모델 성능: 0.8882386857482165
선택된 피처 개수: 51, 모델 성능: 0.8880407562627471
선택된 피처 개수: 50, 모델 성능: 0.8870511088354002
선택된 피처 개수: 49, 모델 성능: 0.8870338975757941
선택된 피처 개수: 48, 모델 성능: 0.8875588409937781
선택된 피처 개수: 47, 모델 성능: 0.886448714749189
선택된 피처 개수: 46, 모델 성능: 0.8882817138972315
선택된 피처 개수: 45, 모델 성능: 0.8874297565467328
선택된 피처 개수: 44, 모델 성능: 0.8793146476424877
선택된 피처 개수: 43, 모델 성능: 0.8733939743380119
선택된 피처 개수: 42, 모델 성능: 0.8735574813042692
선택된 피처 개수: 41, 모델 성능: 0.8631446692426186
선택된 피처 개수: 40, 모델 성능: 0.863893359035481
선택된 피처 개수: 39, 모델 성능: 0.8640568660017384
선택된 피처 개수: 38, 모델 성능: 0.864857189573419
선택된 피처 개수: 37, 모델 성능: 0.8440918048587386
선택된 피처 개수: 36, 모델 성능: 0.8430591292823766
선택된 피처 개수: 35, 모델 성능: 0.8451933254735248
선택된 피처 개수: 34, 모델 성능: 0.843291481287058
선택된 피처 개수: 33, 모델 성능: 0.8444188187912532
선택된 피처 개수: 32, 모델 성능: 0.8445737201277076
선택된 피처 개수: 31, 모델 성능: 0.7874925776442949
선택된 피처 개수: 30, 모델 성능: 0.7299467311515193
선택된 피처 개수: 29, 모델 성능: 0.7299897593005344
선택된 피처 개수: 28, 모델 성능: 0.7284837740850064
선택된 피처 개수: 27, 모델 성능: 0.7286214641618547
선택된 피처 개수: 26, 모델 성능: 0.7283719008975672
선택된 피처 개수: 25, 모델 성능: 0.7277006617729318
선택된 피처 개수: 24, 모델 성능: 0.7268142819032211
선택된 피처 개수: 23, 모델 성능: 0.7256611275096168
선택된 피처 개수: 22, 모델 성능: 0.7252480572790719
선택된 피처 개수: 21, 모델 성능: 0.722761030266
선택된 피처 개수: 20, 모델 성능: 0.7211862000120479
선택된 피처 개수: 19, 모델 성능: 0.721263650680275
선택된 피처 개수: 18, 모델 성능: 0.7207817354113061
선택된 피처 개수: 17, 모델 성능: 0.720755918521897
선택된 피처 개수: 16, 모델 성능: 0.7159625827216165
선택된 피처 개수: 15, 모델 성능: 0.7134755557085445
선택된 피처 개수: 14, 모델 성능: 0.7119265423440014
선택된 피처 개수: 13, 모델 성능: 0.7027873634931973
선택된 피처 개수: 12, 모델 성능: 0.7024001101520615
선택된 피처 개수: 11, 모델 성능: 0.7019698286619106
선택된 피처 개수: 10, 모델 성능: 0.6933039594502723
선택된 피처 개수: 9, 모델 성능: 0.692150805056668
선택된 피처 개수: 8, 모델 성능: 0.6920733543884409
선택된 피처 개수: 7, 모델 성능: 0.6920991712778499
선택된 피처 개수: 6, 모델 성능: 0.6869013708768276
선택된 피처 개수: 5, 모델 성능: 0.6794661067270208
선택된 피처 개수: 4, 모델 성능: 0.5759834083457398
선택된 피처 개수: 3, 모델 성능: 0.5445040145263031
선택된 피처 개수: 2, 모델 성능: 0.5297711763035378
선택된 피처 개수: 1, 모델 성능: 0.5167680696711788
'''






