x = np.array([1, 2, 3]) # 1차원 배열 생성
print(x.ndim) # 해당 배열의 차원 확인
print(x.dtype)  # 데이터 타입을 확인
print(x.shape)     #(715, 2)
print(x.columns)    # 인덱스 칼럼 확인

print(train_csv.isna().sum())           # 데이터 프레임 결측치 확인
print(train_csv.info())         # Non-Null Count : 결측치가 아닌 데이터 갯수
print(test_csv.info())          # 컬럼 이름, 결측을 제외한 값 카운트, 타입을 보여줌.
print(train_csv.describe())  


x = x.transpose() # ex) (2,10) -> (10,2)
                  # 전치행렬 : x = x.T으로 해도 된다.

# batch_size 개념
Batch size(배치 크기)는 신경망 모델을 훈련시킬 때 사용되는 데이터 샘플의 그룹 크기
훈련 데이터를 한 번에 모두 네트워크에 전달하는 것이 아니라 작은 배치로 나눠서 전달함으로써 효율적으로 학습을 수행
일반적으로 데이터셋은 매우 크기 때문에 한 번에 전체 데이터를 처리하는 것은 메모리와 계산 리소스 측면에서 비효율적일 수 있다.
이에 따라 데이터를 작은 배치로 나누어 모델을 반복적으로 업데이트하는 방법이 사용됨
이 과정에서 각 배치에 대한 손실(loss)을 계산하고, 이를 사용하여 모델의 가중치를 조정


from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, 
                                                    train_size=0.7,     # 디폴트 : 0.75
                                                    test_size=0.3,      # 디폴트 : 0.25  // 1이상 오버되면 에러 // 이하는 작동하지만 데이터 손실 O
                                                    random_state=1,     # 이것만 잘 조절해도 성적이 좋을 수 있다. // 데이터 클수록 발휘한다.
                                                    shuffle=True,       # 디폴트 : True  // False : 섞지 않겠다.
                                                    # stratify=         # 층을 이루게 하다, 계층화하다
                                                    )   
평가데이터는 가중치에 영향 x
train_test_split(stratify) : 훈련 데이터와 테스트에서 동일한 비율을 유지 (불균형한 클래스 분포 방지)

# 그림 그리는 API 땡겨오기
import matplotlib.pyplot as plt
plt.figure(figsize=(9,6))
plt.plot(hist.history['loss'], c='red', label='loss', marker='.')
plt.plot(hist.history['val_loss'], c='blue', label='val_loss', marker='.')
plt.legend(loc='upper right') # 라벨
plt.title('따릉이 LOSS') #제목
plt.xlabel('epoch')
plt.ylabel('loss')
plt.grid()
plt.show()

plt.scatter(x, y)   # 흩뿌리다. 
plt.plot(x, results, color='red')
plt.show()
# x(파란선) :  실제 데이터 // results(빨간선) : 예측 데이터


import time
start_time = time.time()   #현재 시간
end_time = time.time()   #끝나는 시간
print("걸린시간 : ", round(end_time - start_time, 2),"초")


############## 결측치 처리, 1.제거 ############
# print(train_csv.isnull().sum())       
print(train_csv.isna().sum())           # 데이터 프레임 결측치 확인
train_csv = train_csv.dropna()          # 결측치있으면 행이 삭제됨
print(train_csv.info())
############## 결측치 처리, 2.채움 ############
test_csv = test_csv.fillna(test_csv.mean())     # train에서 없어진 결측치를 평균인 중간값으로 채움.
print(test_csv.info())
############ x 와 y를 분리 ################
x = train_csv.drop(['count'], axis=1)       # 행삭제 : axis = 0 // 열삭제 : axis = 1 // train_csv에 있는 'count'열 삭제 
print(x)
y = train_csv['count']                      # train_csv에 있는 'count'열을 y로 설정
print(y)

print(np.unique(y, return_counts=True)) # (array([3, 4, 5, 6, 7, 8, 9], dtype=int64), array([  26,  186, 1788, 2416,  924,  152,    5], dtype=int64))
print(pd.value_counts(y))
6    2416
5    1788
7     924
4     186
8     152
3      26
9       5



-----------------평가 지표-----------------
from sklearn.metrics import r2_score
r2 = r2_score(y_test, y_predict)    # 실제값, 예측값
print("r2 스코어 : " , r2)

r2_score는 회귀 모델의 성능을 측정하는 지표
R²는 예측 모델이 주어진 데이터를 얼마나 잘 설명
R² 값은 0에서 1 사이의 범위에 있으며, 높을수록 모델이 데이터를 더 잘 설명한다는 것을 나타낸다.

f1 = f1_score(y_test, y_predict)
F_score는 정밀도(precision)와 재현율(recall)의 조화평균으로 계산되는 지표

y_predict = model.predict(x_test)
def RMSE(y_test, y_predict):
    return np.sqrt(mean_squared_error(y_test, y_predict))
rmse = RMSE(y_test, y_predict)
print("RMSE : ", rmse)  # rmse : 평균 제곱근 오차
print("MSE : ", loss)   # mse : 평균 제곱 오차

loss = 'binary_crossentropy' -> 이진 분류때 씀 (sigmoid)
loss = 'categorical_crossentropy' -> 다중 클래스 분류에서 주로 사용 (softmax)




#1. 원핫엔코딩 keras 
from keras.utils import to_categorical # 10진수를 2진수로 바꿔주는 함수
y_ohe = to_categorical(y)
print(y_ohe)
print(y_ohe.shape)  # (178, 3)

#2. 원핫엔코딩 판다스
y_ohe = pd.get_dummies(y, dtype='int')
print(y_ohe)
print(y_ohe.shape)  # (178, 3)


#3. 원핫엔코딩 사이킷런
from sklearn.preprocessing import OneHotEncoder
y = y.reshape(-1, 1)    
#  차원을 재구조화 및 변경하고자 할 때 reshape() 함수를 사용
# 원래 y는 (178,) --> reshape로 (178, 1)
# - reshape(-1, 정수) 일때
# : 행 자리에 -1, 그리고 열 위치에 임의의 정수가 있을 때 정수에 따라서 178개의 원소가 해당 열 개수만큼 자동으로 구조화
print(y.shape)  # (178, 1)
ohe = OneHotEncoder(sparse=True)
y = ohe.fit_transform(y).toarray()  # toarray : 넘파이 배열로 변환하는 메서드
print(y)
print(y.shape)  # (178, 3)

[0, 1, 0]이 One-Hot 인코딩된 벡터라면 argmax를 사용하면
두 번째 위치에 해당하는 인덱스 1이 반환


# 데이터 전처리
train_csv['type'] = train_csv['type'].map({"white":1, "red":0}).astype(int)
test_csv['type'] = test_csv['type'].replace({"white":1, "red":0}).astype(int)
# map은 주로 시리즈에 적용, replace는 데이터프레임 또는 시리즈에 대해 전체적으로 적용

model.save("..\_data\_save\keras24_save_model.h5") # 상대 경로//. 한개는 그 위치 저장 / .. 두개는 상위 폴더 저장
model = load_model('..\_data\_save\keras24_save_model.h5')
model.save_weights("..\_data\_save\keras24_5_save_model2.h5")
model.load_weights("..\_data\_save\keras24_5_save_model2.h5")   # 모델이 없어서 불러오든가 구성되어있어야한다.

mcp = ModelCheckpoint(monitor='val_loss',
                      mode='auto',
                      verbose=1,
                      save_best_only=True,
                      filepath='../_data/_save/MCP/keras25_MCP1.hdf5',
                      )
# 검증 손실이 감소할 때만 모델이 저장, 과적합을 방지하고 최적의 모델 저장






fit_transform( )과 transform( )의 차이
스케일링을 할 때 꼭 사용하는게 fit_transform(), fit(), transform()메서드이다. 
fit_transform()은 말그대로 fit()과 transform()을 한번에 처리할 수 있게 하는 메서드인데 조심해야 하는 것은 테스트 데이터에는 fit_transform()메서드를 쓰면 안된다.
fit()은 데이터를 학습시키는 메서드이고 transform()은 실제로 학습시킨 것을 적용하는 메서드이다.

fit 메서드는 주어진 데이터에서 평균과 표준편차를 계산하여 학습
transform 메서드를 통해 학습된 평균과 표준편차를 적용하여 데이터 표준화



데이터 스케일링(Data Scaling)

1. StandardScaler
feature 값의 평균=0, 표준편차=1로 조정해서 모든 특성이 같은 크기를 갖게 정규화 한다. 각 데이터가 평균에서 몇 표준편차만큼 떨어져있는지를 기준으로 삼게 된다. 평균을 제거하고 데이터를 단위 분산으로 조정하기 때문에 이상치가 있다면 평균과 표준편차에 영향을 미쳐 변환된 데이터의 확산은 매우 달라지게 된다.
데이터의 특징을 모르는 경우 가장 무난한 종류의 정규화 중 하나

2. MinMaxScaler
최대값=1, 최소값=0으로 조정하여 0~1 구간 내에 균등하게 값을 배정하는 정규화 방법. 다만 이상치가 있는 경우 변환된 값이 매우 좁은 범위로 압축될 수 있어 아웃라이어에 취약함. 
(만약 음수 값이 있으면 -1에서 1 값으로 변환)

3. MaxAbsScaler
0을 기준으로 절대값이 가장 큰 수가 1 또는 -1이 되도록 조정, 양수 데이터로만 구성되어 MinMaxScaler와 유사하게 동작하며 데이터셋에서는 아웃라이어에 민감
데이터 스케일링을 하는 이유가 아웃라이어의 영향을 최소화하는 것이기 때문에 보통은 이상치 영향을 가장 적게 받는 StandardScaler 혹은 RobustScaler를 주로 사용한다. 

4. RobustScaler
중앙값=0, IQR(1분위~3분위값)=1로 조정, 아웃라이어 영향을 최소화하며 정규분포보다 더 넓게 분포. 이상치에 강함.
RobustScaler를 사용하면 모든 변수들이 같은 스케일을 갖게 되며, StandardScaler에 비해 스케일링 결과가 더 넓은 범위로 분포
IQR = Q3 - Q1 : 즉, 25퍼센타일과 75퍼센타일의 값들을 다룬다.


ModelCheckpoint()
훈련하는 동안 일정한 간격으로 모델의 체크포인트를 저장
save_best_only=True 최상의 검증 세트 점수에서만 모델을 저장



import datetime
date = datetime.datetime.now()  
print(date) # 2024-01-17 10:55:11.015537
print(type(date))   # <class 'datetime.datetime'> 시간 데이터
date = date.strftime("%m%d_%H%M")   # "%m%d_%H%M" 월 일 시간 분 // _는 문자
print(date) # 0117_1059
print(type(date))   # <class 'str'> 문자열
%m : 0을 채운 두 자리 수의 월 ex) 01, 02 ...  11 ,12
%d : 0을 채운 두 자리 수의 일 ex) 01, 02 ...  30, 31
%H : 0을 채운 24시간제의 시간 ex) 00, 01 … 23
%M : 0을 채운 두 자리 수의 분 ex) 00, 01 ... 58, 59

path = '../_data/_save/MCP/'  # 문자열로 저장
filename = '{epoch:04d}-{val_loss:.4f}.hdf5'  # 히스토리로 반환 되는 놈들 // 훈련 횟수 - 발로스 // 04d : 4자리수 까지  // 04f : 소수 4번째 자리 까지 // ex) 1000-0.3333.hdf5
filepath = "".join([path, 'k25_', date, '_', filename])  # ""은 더하기 개념 (path + date + filename)
# ../_data/_save/MCP/k25_0117_1059_0001-0.3333.hdf5


DNN (Deep Neural Network):
개념: DNN은 입력 레이어, 은닉 레이어(여러 층 포함), 출력 레이어로 구성된 인공신경망 구조. 각 뉴런은 입력과 가중치를 곱하고 활성화 함수를 통과시켜 출력을 생성.
특징:
DNN의 각 층은 이전 층의 모든 뉴런과 연결되어 있다.
데이터는 입력에서 출력 방향으로 한 방향으로만 전달.


CNN (Convolutional Neural Network):
이미지의 한 픽셀과 주변 픽셀들의 연관 관계를 통해 학습시키는 것
먼저 반복적으로 Layer를 쌓으며 특징을 찾는 ①특징 추출 부분(Convolution + Pooling layer)과 이미지를 분류하는 부분(FC layer → Softmax함수 적용)으로 나뉜다.
임의의 값을 가지는 n*n 크기의 필터를 convolution연산을 하면서 원본 데이터를 입축시키는 원리
(stride값만큼 filter를 이동시키면서 겹치는 부분의 각 원소값을 곱해서 모두 더한 값을 출력)
출력 데이터 크기는 Filter 크기, Stride, Padding, Pooling으로 결정
출력 데이터 채널 = Filter의 개수로 결정

#2. 모델
model = Sequential()
model.add(Conv2D(8, 
                 (2,2), # 커넬(kernel): 한번에 처리할 노드의 크기 
                 strides=1, # stride : 커널의 보폭
                 input_shape=(28, 28, 1),   #
                 padding='same',))  # 모양 같이 하고 싶으면 padding : 'same'쓴다. 'valid'은 디폴트
                        
model.add(Conv2D(filters=7, kernel_size=(2,2)))
model.add(Conv2D(15, (2,2), padding='same') )
model.add(MaxPooling2D())   # 데이터의 특징점만을 뽑아내 축소 // 디폴트값 N분의  // 연산량 0 // 데이터 많을때 쓰는게 좋긴함. 데이터 작으면 날아갈수도,, 
# maxpooling
# 원본 이미지 그대로 사용할 경우, 
# 연산량이 너무 많고 컴퓨터 메모리 크기는 한정돼 있으므로 적당히 압축해서 중요한 정보만 남겨 특징을 추출하고 차원을 줄여주는 것 
# (데이터 사이즈 축소, 과적합(Overfitting)문제 방지)

model.add(Flatten())    # 입력 데이터를 1차원으로 평탄화. 2D 혹은 3D의 특징 맵(feature map)을 1D 벡터로 변환, 이후의 레이어에서 처리하기 쉽게 만들어주는 역할 // reshape랑 비슷한 개념
model.add(Dense(units=8)) # 주로 2차원 받음 
model.add(Dense(7, input_shape=(8, )))
#                   shape=(batch_size, input_dim)
model.add(Dense(6))
model.add(Dense(10, activation='softmax'))


Feature map [특성맵] = Activation map

Convolution layer의 입력 데이터가 필터를 통과하며 합성곱을 통해 만든 출력 데이터
입력에 있는 특징을 잡아냈다는 의미로 특성맵이라고 부름
입력에서의 차원을 유지하면서 계속 합성곱을 진행하는 특징이 있음 (입력 데이터가 2차원이면 특성맵도 2차원)
Activation map은 Feature map 행렬에 활성화함수를 적용한 최종 출력 결과를 말함


x_train = x_train.astype(np.float32)/255.0
x_test = x_test.astype(np.float32)/255.0
이미지 데이터의 픽셀 값은 일반적으로 0부터 255까지의 값을 가진다. 
이 값을 255로 나누면 픽셀 값이 0에서 1 사이의 범위로 스케일링되게 된다.

ImageDataGenerator # 이미지를 0~255의 값으로 정규화된 배열로 변환 // 이미지 숫자로 바꿔줌 (컴퓨터가 알아먹을수있게) // 데이터 수치화, 변환하는 도구

xy_train = train_datagen.flow_from_directory
# DirectoryIterator 여기서 x는 (배치 크기, *표적 크기, 채널)의 형태의 이미지 배치로 구성된 numpy 배열이고 y는 그에 대응하는 라벨로 이루어진 numpy 배열
# x와 y가 합쳐져있는 형태









